2018-11-06 06:26:15 INFO  CoarseGrainedExecutorBackend:2611 - Started daemon with process name: 1894@eda27e32a980
2018-11-06 06:26:15 INFO  SignalUtils:54 - Registered signal handler for TERM
2018-11-06 06:26:15 INFO  SignalUtils:54 - Registered signal handler for HUP
2018-11-06 06:26:15 INFO  SignalUtils:54 - Registered signal handler for INT
2018-11-06 06:26:15 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing view acls to: yields,root
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing modify acls to: yields,root
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing view acls groups to: 
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing modify acls groups to: 
2018-11-06 06:26:15 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yields, root); groups with view permissions: Set(); users  with modify permissions: Set(yields, root); groups with modify permissions: Set()
2018-11-06 06:26:15 INFO  TransportClientFactory:267 - Successfully created connection to 2fe848681309/172.18.0.14:34693 after 62 ms (0 ms spent in bootstraps)
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing view acls to: yields,root
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing modify acls to: yields,root
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing view acls groups to: 
2018-11-06 06:26:15 INFO  SecurityManager:54 - Changing modify acls groups to: 
2018-11-06 06:26:15 INFO  SecurityManager:54 - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(yields, root); groups with view permissions: Set(); users  with modify permissions: Set(yields, root); groups with modify permissions: Set()
2018-11-06 06:26:15 INFO  TransportClientFactory:267 - Successfully created connection to 2fe848681309/172.18.0.14:34693 after 2 ms (0 ms spent in bootstraps)
2018-11-06 06:26:16 INFO  DiskBlockManager:54 - Created local directory at /tmp/spark-bc2c70fc-6c11-4d8d-8dcf-6a6612d00fec/executor-7999704c-54b7-4cd9-a468-b2da49716ecb/blockmgr-3ad5e028-7bf6-4d9b-a952-5c87ffbe5773
2018-11-06 06:26:16 INFO  MemoryStore:54 - MemoryStore started with capacity 15.8 GB
2018-11-06 06:26:16 INFO  CoarseGrainedExecutorBackend:54 - Connecting to driver: spark://CoarseGrainedScheduler@2fe848681309:34693
2018-11-06 06:26:16 INFO  WorkerWatcher:54 - Connecting to worker spark://Worker@172.18.0.9:32985
2018-11-06 06:26:16 INFO  TransportClientFactory:267 - Successfully created connection to /172.18.0.9:32985 after 1 ms (0 ms spent in bootstraps)
2018-11-06 06:26:16 INFO  WorkerWatcher:54 - Successfully connected to spark://Worker@172.18.0.9:32985
2018-11-06 06:26:16 INFO  CoarseGrainedExecutorBackend:54 - Successfully registered with driver
2018-11-06 06:26:16 INFO  Executor:54 - Starting executor ID 0 on host 172.18.0.9
2018-11-06 06:26:16 INFO  Utils:54 - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37977.
2018-11-06 06:26:16 INFO  NettyBlockTransferService:54 - Server created on 172.18.0.9:37977
2018-11-06 06:26:16 INFO  BlockManager:54 - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-11-06 06:26:16 INFO  BlockManagerMaster:54 - Registering BlockManager BlockManagerId(0, 172.18.0.9, 37977, None)
2018-11-06 06:26:16 INFO  BlockManagerMaster:54 - Registered BlockManager BlockManagerId(0, 172.18.0.9, 37977, None)
2018-11-06 06:26:16 INFO  BlockManager:54 - Initialized BlockManager: BlockManagerId(0, 172.18.0.9, 37977, None)
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 0
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 1
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 2
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 3
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 4
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 5
2018-11-06 06:26:27 INFO  Executor:54 - Running task 1.0 in stage 0.0 (TID 1)
2018-11-06 06:26:27 INFO  Executor:54 - Running task 4.0 in stage 0.0 (TID 4)
2018-11-06 06:26:27 INFO  Executor:54 - Running task 2.0 in stage 0.0 (TID 2)
2018-11-06 06:26:27 INFO  Executor:54 - Running task 3.0 in stage 0.0 (TID 3)
2018-11-06 06:26:27 INFO  Executor:54 - Running task 0.0 in stage 0.0 (TID 0)
2018-11-06 06:26:27 INFO  Executor:54 - Running task 5.0 in stage 0.0 (TID 5)
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 6
2018-11-06 06:26:27 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 7
2018-11-06 06:26:27 INFO  Executor:54 - Running task 6.0 in stage 0.0 (TID 6)
2018-11-06 06:26:27 INFO  Executor:54 - Running task 7.0 in stage 0.0 (TID 7)
2018-11-06 06:26:27 INFO  TorrentBroadcast:54 - Started reading broadcast variable 1
2018-11-06 06:26:27 INFO  TransportClientFactory:267 - Successfully created connection to 2fe848681309/172.18.0.14:41059 after 1 ms (0 ms spent in bootstraps)
2018-11-06 06:26:27 INFO  MemoryStore:54 - Block broadcast_1_piece0 stored as bytes in memory (estimated size 57.8 KB, free 15.8 GB)
2018-11-06 06:26:27 INFO  TorrentBroadcast:54 - Reading broadcast variable 1 took 96 ms
2018-11-06 06:26:27 INFO  MemoryStore:54 - Block broadcast_1 stored as values in memory (estimated size 174.4 KB, free 15.8 GB)
2018-11-06 06:26:28 INFO  CodeGenerator:54 - Code generated in 292.778458 ms
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=22/part-00002-bf35044a-fffd-40a5-a43e-9c52af3b1eb4.c000.snappy.parquet, range: 0-5312487, partition values: [22]
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=25/part-00000-0d83c239-53cd-41fb-8ee8-a6c539977c27.c000.snappy.parquet, range: 0-420621, partition values: [25]
2018-11-06 06:26:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00001-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6671878, partition values: [3]
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00003-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6667934, partition values: [3]
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=11/part-00000-de6cc0fb-e96e-4687-ab75-22f1461565dc.c000.snappy.parquet, range: 0-99914, partition values: [11]
2018-11-06 06:26:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=15/part-00000-eba73229-678b-412b-9a56-93a05c6fa933.c000.snappy.parquet, range: 0-7768490, partition values: [15]
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=13/part-00000-1e67ae2c-ec2b-49a9-9907-c9e91e830c6b.c000.snappy.parquet, range: 0-3442246, partition values: [13]
2018-11-06 06:26:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:28 INFO  TorrentBroadcast:54 - Started reading broadcast variable 0
2018-11-06 06:26:28 INFO  MemoryStore:54 - Block broadcast_0_piece0 stored as bytes in memory (estimated size 25.7 KB, free 15.8 GB)
2018-11-06 06:26:28 INFO  TorrentBroadcast:54 - Reading broadcast variable 0 took 17 ms
2018-11-06 06:26:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:26:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:26:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:26:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:26:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:26:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:26:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:26:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:26:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:26:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:26:28 INFO  MemoryStore:54 - Block broadcast_0 stored as values in memory (estimated size 334.3 KB, free 15.8 GB)
2018-11-06 06:26:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:26:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062628_0000_m_000000_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:26:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062628_0000_m_000000_0
2018-11-06 06:26:28 ERROR FileFormatWriter:70 - Job job_20181106062628_0000 aborted.
2018-11-06 06:26:28 ERROR Executor:91 - Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062628_0000_m_000000_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:26:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 8
2018-11-06 06:26:28 INFO  Executor:54 - Running task 0.1 in stage 0.0 (TID 8)
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:26:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:26:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:26:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:26:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:26:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:26:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:26:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:26:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:26:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:26:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:26:29 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062628_0000_m_000000_1 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:26:29 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062628_0000_m_000000_1
2018-11-06 06:26:29 ERROR FileFormatWriter:70 - Job job_20181106062628_0000 aborted.
2018-11-06 06:26:29 ERROR Executor:91 - Exception in task 0.1 in stage 0.0 (TID 8)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062628_0000_m_000000_1 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:26:29 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 9
2018-11-06 06:26:29 INFO  Executor:54 - Running task 0.2 in stage 0.0 (TID 9)
2018-11-06 06:26:29 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:29 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:29 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:29 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:29 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:29 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:29 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:26:29 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:26:29 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:26:29 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:26:29 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:26:29 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:26:29 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:26:29 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:26:29 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:26:29 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:26:29 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:26:29 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062629_0000_m_000000_2 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:26:29 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062629_0000_m_000000_2
2018-11-06 06:26:29 ERROR FileFormatWriter:70 - Job job_20181106062629_0000 aborted.
2018-11-06 06:26:29 ERROR Executor:91 - Exception in task 0.2 in stage 0.0 (TID 9)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062629_0000_m_000000_2 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:26:29 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 10
2018-11-06 06:26:29 INFO  Executor:54 - Running task 0.3 in stage 0.0 (TID 10)
2018-11-06 06:26:29 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:29 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:29 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:26:29 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:26:29 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:29 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:26:29 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:26:29 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:26:29 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:26:29 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:26:29 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:26:29 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:26:29 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:26:29 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:26:29 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:26:29 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:26:29 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:26:29 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062629_0000_m_000000_3 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:26:29 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062629_0000_m_000000_3
2018-11-06 06:26:29 ERROR FileFormatWriter:70 - Job job_20181106062629_0000 aborted.
2018-11-06 06:26:29 ERROR Executor:91 - Exception in task 0.3 in stage 0.0 (TID 10)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data.parquet/_temporary/0/_temporary/attempt_20181106062629_0000_m_000000_3 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:26:29 INFO  Executor:54 - Executor is trying to kill task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor is trying to kill task 5.0 in stage 0.0 (TID 5), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor is trying to kill task 2.0 in stage 0.0 (TID 2), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor is trying to kill task 6.0 in stage 0.0 (TID 6), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor is trying to kill task 3.0 in stage 0.0 (TID 3), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor is trying to kill task 7.0 in stage 0.0 (TID 7), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor is trying to kill task 4.0 in stage 0.0 (TID 4), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor killed task 2.0 in stage 0.0 (TID 2), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor killed task 4.0 in stage 0.0 (TID 4), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor killed task 7.0 in stage 0.0 (TID 7), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor killed task 1.0 in stage 0.0 (TID 1), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor killed task 5.0 in stage 0.0 (TID 5), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor killed task 6.0 in stage 0.0 (TID 6), reason: Stage cancelled
2018-11-06 06:26:29 INFO  Executor:54 - Executor killed task 3.0 in stage 0.0 (TID 3), reason: Stage cancelled
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 11
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 12
2018-11-06 06:28:49 INFO  Executor:54 - Running task 0.0 in stage 1.0 (TID 11)
2018-11-06 06:28:49 INFO  Executor:54 - Running task 1.0 in stage 1.0 (TID 12)
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 13
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 14
2018-11-06 06:28:49 INFO  Executor:54 - Running task 2.0 in stage 1.0 (TID 13)
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 15
2018-11-06 06:28:49 INFO  Executor:54 - Running task 3.0 in stage 1.0 (TID 14)
2018-11-06 06:28:49 INFO  Executor:54 - Running task 4.0 in stage 1.0 (TID 15)
2018-11-06 06:28:49 INFO  TorrentBroadcast:54 - Started reading broadcast variable 3
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 16
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 17
2018-11-06 06:28:49 INFO  Executor:54 - Running task 5.0 in stage 1.0 (TID 16)
2018-11-06 06:28:49 INFO  Executor:54 - Running task 6.0 in stage 1.0 (TID 17)
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 18
2018-11-06 06:28:49 INFO  Executor:54 - Running task 7.0 in stage 1.0 (TID 18)
2018-11-06 06:28:49 INFO  MemoryStore:54 - Block broadcast_3_piece0 stored as bytes in memory (estimated size 57.8 KB, free 15.8 GB)
2018-11-06 06:28:49 INFO  TorrentBroadcast:54 - Reading broadcast variable 3 took 41 ms
2018-11-06 06:28:49 INFO  MemoryStore:54 - Block broadcast_3 stored as values in memory (estimated size 174.4 KB, free 15.8 GB)
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=11/part-00000-de6cc0fb-e96e-4687-ab75-22f1461565dc.c000.snappy.parquet, range: 0-99914, partition values: [11]
2018-11-06 06:28:49 INFO  TorrentBroadcast:54 - Started reading broadcast variable 2
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=22/part-00002-bf35044a-fffd-40a5-a43e-9c52af3b1eb4.c000.snappy.parquet, range: 0-5312487, partition values: [22]
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00003-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6667934, partition values: [3]
2018-11-06 06:28:49 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00001-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6671878, partition values: [3]
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=15/part-00000-eba73229-678b-412b-9a56-93a05c6fa933.c000.snappy.parquet, range: 0-7768490, partition values: [15]
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  MemoryStore:54 - Block broadcast_2_piece0 stored as bytes in memory (estimated size 25.7 KB, free 15.8 GB)
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=25/part-00000-0d83c239-53cd-41fb-8ee8-a6c539977c27.c000.snappy.parquet, range: 0-420621, partition values: [25]
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_0
2018-11-06 06:28:49 INFO  TorrentBroadcast:54 - Reading broadcast variable 2 took 20 ms
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=13/part-00000-1e67ae2c-ec2b-49a9-9907-c9e91e830c6b.c000.snappy.parquet, range: 0-3442246, partition values: [13]
2018-11-06 06:28:49 ERROR Executor:91 - Exception in task 0.0 in stage 1.0 (TID 11)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 19
2018-11-06 06:28:49 INFO  MemoryStore:54 - Block broadcast_2 stored as values in memory (estimated size 334.3 KB, free 15.8 GB)
2018-11-06 06:28:49 INFO  Executor:54 - Running task 0.1 in stage 1.0 (TID 19)
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_1 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_1
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 ERROR Executor:91 - Exception in task 0.1 in stage 1.0 (TID 19)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_1 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 20
2018-11-06 06:28:49 INFO  Executor:54 - Running task 0.2 in stage 1.0 (TID 20)
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_2 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_2
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 ERROR Executor:91 - Exception in task 0.2 in stage 1.0 (TID 20)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_2 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:28:49 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 21
2018-11-06 06:28:49 INFO  CodecPool:181 - Got brand-new decompressor [.snappy]
2018-11-06 06:28:49 INFO  Executor:54 - Running task 0.3 in stage 1.0 (TID 21)
2018-11-06 06:28:49 INFO  CodecPool:181 - Got brand-new decompressor [.snappy]
2018-11-06 06:28:49 INFO  CodecPool:181 - Got brand-new decompressor [.snappy]
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:28:49 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 INFO  CodecPool:181 - Got brand-new decompressor [.snappy]
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_3 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_3
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 ERROR Executor:91 - Exception in task 0.3 in stage 1.0 (TID 21)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000000_3 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:28:49 INFO  CodecPool:181 - Got brand-new decompressor [.snappy]
2018-11-06 06:28:49 INFO  Executor:54 - Executor is trying to kill task 4.0 in stage 1.0 (TID 15), reason: Stage cancelled
2018-11-06 06:28:49 INFO  CodecPool:181 - Got brand-new decompressor [.snappy]
2018-11-06 06:28:49 INFO  Executor:54 - Executor is trying to kill task 1.0 in stage 1.0 (TID 12), reason: Stage cancelled
2018-11-06 06:28:49 INFO  Executor:54 - Executor is trying to kill task 5.0 in stage 1.0 (TID 16), reason: Stage cancelled
2018-11-06 06:28:49 INFO  Executor:54 - Executor is trying to kill task 2.0 in stage 1.0 (TID 13), reason: Stage cancelled
2018-11-06 06:28:49 INFO  Executor:54 - Executor is trying to kill task 6.0 in stage 1.0 (TID 17), reason: Stage cancelled
2018-11-06 06:28:49 INFO  Executor:54 - Executor is trying to kill task 7.0 in stage 1.0 (TID 18), reason: Stage cancelled
2018-11-06 06:28:49 INFO  Executor:54 - Executor is trying to kill task 3.0 in stage 1.0 (TID 14), reason: Stage cancelled
2018-11-06 06:28:49 INFO  CodecPool:181 - Got brand-new decompressor [.snappy]
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000006_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000006_0
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 INFO  Executor:54 - Executor interrupted and killed task 6.0 in stage 1.0 (TID 17), reason: Stage cancelled
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000005_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000003_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000005_0
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000003_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:28:49 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:28:49 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:28:49 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:28:49 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:28:49 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:28:49 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:28:49 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000007_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 INFO  Executor:54 - Executor interrupted and killed task 3.0 in stage 1.0 (TID 14), reason: Stage cancelled
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 INFO  Executor:54 - Executor interrupted and killed task 5.0 in stage 1.0 (TID 16), reason: Stage cancelled
2018-11-06 06:28:49 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000007_0
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 INFO  Executor:54 - Executor interrupted and killed task 7.0 in stage 1.0 (TID 18), reason: Stage cancelled
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000002_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000002_0
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000001_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000004_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000004_0
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data2.parquet/_temporary/0/_temporary/attempt_20181106062849_0001_m_000001_0
2018-11-06 06:28:49 INFO  Executor:54 - Executor interrupted and killed task 2.0 in stage 1.0 (TID 13), reason: Stage cancelled
2018-11-06 06:28:49 ERROR FileFormatWriter:70 - Job job_20181106062849_0001 aborted.
2018-11-06 06:28:49 INFO  Executor:54 - Executor interrupted and killed task 4.0 in stage 1.0 (TID 15), reason: Stage cancelled
2018-11-06 06:28:49 INFO  Executor:54 - Executor interrupted and killed task 1.0 in stage 1.0 (TID 12), reason: Stage cancelled
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 22
2018-11-06 06:29:28 INFO  Executor:54 - Running task 0.0 in stage 2.0 (TID 22)
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 23
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 24
2018-11-06 06:29:28 INFO  Executor:54 - Running task 1.0 in stage 2.0 (TID 23)
2018-11-06 06:29:28 INFO  Executor:54 - Running task 2.0 in stage 2.0 (TID 24)
2018-11-06 06:29:28 INFO  TorrentBroadcast:54 - Started reading broadcast variable 5
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 25
2018-11-06 06:29:28 INFO  Executor:54 - Running task 3.0 in stage 2.0 (TID 25)
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 26
2018-11-06 06:29:28 INFO  Executor:54 - Running task 4.0 in stage 2.0 (TID 26)
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 27
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 28
2018-11-06 06:29:28 INFO  Executor:54 - Running task 5.0 in stage 2.0 (TID 27)
2018-11-06 06:29:28 INFO  Executor:54 - Running task 6.0 in stage 2.0 (TID 28)
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 29
2018-11-06 06:29:28 INFO  Executor:54 - Running task 7.0 in stage 2.0 (TID 29)
2018-11-06 06:29:28 INFO  MemoryStore:54 - Block broadcast_5_piece0 stored as bytes in memory (estimated size 57.8 KB, free 15.8 GB)
2018-11-06 06:29:28 INFO  TorrentBroadcast:54 - Reading broadcast variable 5 took 16 ms
2018-11-06 06:29:28 INFO  MemoryStore:54 - Block broadcast_5 stored as values in memory (estimated size 174.4 KB, free 15.8 GB)
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=11/part-00000-de6cc0fb-e96e-4687-ab75-22f1461565dc.c000.snappy.parquet, range: 0-99914, partition values: [11]
2018-11-06 06:29:28 INFO  TorrentBroadcast:54 - Started reading broadcast variable 4
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=25/part-00000-0d83c239-53cd-41fb-8ee8-a6c539977c27.c000.snappy.parquet, range: 0-420621, partition values: [25]
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00003-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6667934, partition values: [3]
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=13/part-00000-1e67ae2c-ec2b-49a9-9907-c9e91e830c6b.c000.snappy.parquet, range: 0-3442246, partition values: [13]
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=22/part-00002-bf35044a-fffd-40a5-a43e-9c52af3b1eb4.c000.snappy.parquet, range: 0-5312487, partition values: [22]
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_0
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 0.0 in stage 2.0 (TID 22)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00001-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6671878, partition values: [3]
2018-11-06 06:29:28 INFO  MemoryStore:54 - Block broadcast_4_piece0 stored as bytes in memory (estimated size 25.7 KB, free 15.8 GB)
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=15/part-00000-eba73229-678b-412b-9a56-93a05c6fa933.c000.snappy.parquet, range: 0-7768490, partition values: [15]
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 30
2018-11-06 06:29:28 INFO  Executor:54 - Running task 0.1 in stage 2.0 (TID 30)
2018-11-06 06:29:28 INFO  TorrentBroadcast:54 - Reading broadcast variable 4 took 29 ms
2018-11-06 06:29:28 INFO  MemoryStore:54 - Block broadcast_4 stored as values in memory (estimated size 334.3 KB, free 15.8 GB)
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_1 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_1
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 0.1 in stage 2.0 (TID 30)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_1 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 31
2018-11-06 06:29:28 INFO  Executor:54 - Running task 0.2 in stage 2.0 (TID 31)
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_2 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_2
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 0.2 in stage 2.0 (TID 31)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_2 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 32
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  Executor:54 - Running task 0.3 in stage 2.0 (TID 32)
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000006_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000006_0
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 6.0 in stage 2.0 (TID 28)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000006_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000007_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 33
2018-11-06 06:29:28 INFO  Executor:54 - Running task 6.1 in stage 2.0 (TID 33)
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000007_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_3 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_3
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 0.3 in stage 2.0 (TID 32)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000000_3 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 7.0 in stage 2.0 (TID 29)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000007_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=25/part-00000-0d83c239-53cd-41fb-8ee8-a6c539977c27.c000.snappy.parquet, range: 0-420621, partition values: [25]
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000004_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 34
2018-11-06 06:29:28 INFO  Executor:54 - Running task 7.1 in stage 2.0 (TID 34)
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000004_0
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 4.0 in stage 2.0 (TID 26)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000004_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileOutputCommitter:108 - File Output Committer Algorithm version is 1
2018-11-06 06:29:28 INFO  SQLHadoopMapReduceCommitProtocol:54 - Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter
2018-11-06 06:29:28 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=11/part-00000-de6cc0fb-e96e-4687-ab75-22f1461565dc.c000.snappy.parquet, range: 0-99914, partition values: [11]
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:28 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000002_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:28 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000002_0
2018-11-06 06:29:28 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:28 ERROR Executor:91 - Exception in task 2.0 in stage 2.0 (TID 24)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:285)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000002_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	... 8 more
2018-11-06 06:29:28 INFO  Executor:54 - Executor is trying to kill task 6.1 in stage 2.0 (TID 33), reason: Stage cancelled
2018-11-06 06:29:28 INFO  Executor:54 - Executor is trying to kill task 5.0 in stage 2.0 (TID 27), reason: Stage cancelled
2018-11-06 06:29:28 INFO  Executor:54 - Executor is trying to kill task 7.1 in stage 2.0 (TID 34), reason: Stage cancelled
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:28 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:28 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:28 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:28 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:28 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:28 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:28 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:28 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:28 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:28 INFO  Executor:54 - Executor killed task 6.1 in stage 2.0 (TID 33), reason: Stage cancelled
2018-11-06 06:29:28 INFO  Executor:54 - Executor is trying to kill task 2.0 in stage 2.0 (TID 24), reason: Stage cancelled
2018-11-06 06:29:28 INFO  Executor:54 - Executor is trying to kill task 3.0 in stage 2.0 (TID 25), reason: Stage cancelled
2018-11-06 06:29:28 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:29 INFO  Executor:54 - Executor is trying to kill task 1.0 in stage 2.0 (TID 23), reason: Stage cancelled
2018-11-06 06:29:29 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:29 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:29 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:29 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:29 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:29 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:29 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:29 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:29 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:29 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:29 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:29 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:29 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000005_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:29 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:29 INFO  CodecConfig:95 - Compression: SNAPPY
2018-11-06 06:29:29 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000005_0
2018-11-06 06:29:29 INFO  ParquetOutputFormat:329 - Parquet block size to 134217728
2018-11-06 06:29:29 INFO  ParquetOutputFormat:330 - Parquet page size to 1048576
2018-11-06 06:29:29 INFO  ParquetOutputFormat:331 - Parquet dictionary page size to 1048576
2018-11-06 06:29:29 INFO  ParquetOutputFormat:332 - Dictionary is on
2018-11-06 06:29:29 INFO  ParquetOutputFormat:333 - Validation is off
2018-11-06 06:29:29 INFO  ParquetOutputFormat:334 - Writer version is: PARQUET_1_0
2018-11-06 06:29:29 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:29 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:29 INFO  ParquetOutputFormat:335 - Maximum row group padding size is 0 bytes
2018-11-06 06:29:29 INFO  ParquetOutputFormat:336 - Page size checking is: estimated
2018-11-06 06:29:29 INFO  ParquetOutputFormat:337 - Min row count for page size check is: 100
2018-11-06 06:29:29 INFO  ParquetOutputFormat:338 - Max row count for page size check is: 10000
2018-11-06 06:29:29 INFO  Executor:54 - Executor interrupted and killed task 5.0 in stage 2.0 (TID 27), reason: Stage cancelled
2018-11-06 06:29:29 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000001_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:29 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000001_0
2018-11-06 06:29:29 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:29 INFO  Executor:54 - Executor interrupted and killed task 1.0 in stage 2.0 (TID 23), reason: Stage cancelled
2018-11-06 06:29:29 INFO  ParquetWriteSupport:54 - Initialized Parquet WriteSupport with Catalyst schema:
{
  "type" : "struct",
  "fields" : [ {
    "name" : "ObservationDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ObservationSource",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanReference",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OrginalContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentContractId",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ProductClassficicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "BankProductCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UsageTypeCd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentOpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EntryIntoForceDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "EndDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationTypeLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequencyLJR",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CurrentNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OriginalNotionalAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "LoanMaturityDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrevPaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AnnuityAmortizationType",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationFrequency",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AmortizationAmt",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "DayCountType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestPaymentType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestCalculationType",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "InterestRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "MarginalRatePercent",
    "type" : "double",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ReferenceRateCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "NextFixingDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "GrantedAmt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "AccruedInterest",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "UnDrawnExposure",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PaymentFreePeriodCnt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "ApplicationCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "CreditRiskPortfolioCd",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentDt",
    "type" : "string",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "OpeningDt",
    "type" : "timestamp",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "PrepaymentInd",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_record_id",
    "type" : "integer",
    "nullable" : true,
    "metadata" : { }
  }, {
    "name" : "io_yields_convert_corr_id",
    "type" : "long",
    "nullable" : true,
    "metadata" : { }
  } ]
}
and corresponding Parquet message type:
message spark_schema {
  optional int96 ObservationDt;
  optional binary ObservationSource (UTF8);
  optional int64 LoanReference;
  optional binary OrginalContractId (UTF8);
  optional binary CurrentContractId (UTF8);
  optional binary BankCd (UTF8);
  optional binary ProductClassficicationCd (UTF8);
  optional binary BankProductCd (UTF8);
  optional int32 UsageTypeCd;
  optional int96 OriginalOpeningDt;
  optional int96 CurrentOpeningDt;
  optional binary EntryIntoForceDt (UTF8);
  optional int96 EndDt;
  optional int32 AmortizationTypeLJR;
  optional int32 AmortizationFrequencyLJR;
  optional double CurrentNotionalAmt;
  optional double OriginalNotionalAmt;
  optional int96 LoanMaturityDt;
  optional binary NextPaymentDt (UTF8);
  optional binary PrevPaymentDt (UTF8);
  optional binary AmortizationType (UTF8);
  optional int32 AnnuityAmortizationType;
  optional int32 AmortizationFrequency;
  optional double AmortizationAmt;
  optional binary DayCountType (UTF8);
  optional binary InterestPaymentType (UTF8);
  optional binary InterestCalculationType (UTF8);
  optional double InterestRatePercent;
  optional double MarginalRatePercent;
  optional binary ReferenceRateCd (UTF8);
  optional binary NextFixingDt (UTF8);
  optional binary GrantedAmt (UTF8);
  optional binary AccruedInterest (UTF8);
  optional binary UnDrawnExposure (UTF8);
  optional binary PaymentFreePeriodCnt (UTF8);
  optional binary ApplicationCd (UTF8);
  optional binary CreditRiskPortfolioCd (UTF8);
  optional binary PrepaymentDt (UTF8);
  optional int96 OpeningDt;
  optional int32 PrepaymentInd;
  optional int32 io_yields_record_id;
  optional int64 io_yields_convert_corr_id;
}

       
2018-11-06 06:29:29 ERROR Utils:91 - Aborting task
java.io.IOException: Mkdirs failed to create file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000003_0 (exists=false, cwd=file:/home/yields/spark/work/app-20181106062614-0013/0)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:455)
	at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)
	at org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:241)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:342)
	at org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:302)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:37)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.newInstance(ParquetFileFormat.scala:151)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.newOutputWriter(FileFormatWriter.scala:367)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:378)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:269)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:267)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1414)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:272)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:197)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:196)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2018-11-06 06:29:29 WARN  FileOutputCommitter:569 - Could not delete file:/notebook/op_notebooks/calypso_data3.parquet/_temporary/0/_temporary/attempt_20181106062928_0002_m_000003_0
2018-11-06 06:29:29 ERROR FileFormatWriter:70 - Job job_20181106062928_0002 aborted.
2018-11-06 06:29:29 INFO  Executor:54 - Executor interrupted and killed task 3.0 in stage 2.0 (TID 25), reason: Stage cancelled
2018-11-06 06:29:29 INFO  Executor:54 - Executor killed task 7.1 in stage 2.0 (TID 34), reason: Stage cancelled
2018-11-06 08:38:38 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 35
2018-11-06 08:38:38 INFO  Executor:54 - Running task 0.0 in stage 3.0 (TID 35)
2018-11-06 08:38:38 INFO  TorrentBroadcast:54 - Started reading broadcast variable 7
2018-11-06 08:38:38 INFO  MemoryStore:54 - Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.3 KB, free 15.8 GB)
2018-11-06 08:38:38 INFO  TorrentBroadcast:54 - Reading broadcast variable 7 took 16 ms
2018-11-06 08:38:38 INFO  MemoryStore:54 - Block broadcast_7 stored as values in memory (estimated size 35.9 KB, free 15.8 GB)
2018-11-06 08:38:39 INFO  CodeGenerator:54 - Code generated in 117.93755 ms
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=15/part-00001-eba73229-678b-412b-9a56-93a05c6fa933.c000.snappy.parquet, range: 0-7770818, partition values: [15]
2018-11-06 08:38:39 INFO  TorrentBroadcast:54 - Started reading broadcast variable 6
2018-11-06 08:38:39 INFO  MemoryStore:54 - Block broadcast_6_piece0 stored as bytes in memory (estimated size 25.7 KB, free 15.8 GB)
2018-11-06 08:38:39 INFO  TorrentBroadcast:54 - Reading broadcast variable 6 took 6 ms
2018-11-06 08:38:39 INFO  MemoryStore:54 - Block broadcast_6 stored as values in memory (estimated size 334.3 KB, free 15.8 GB)
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=15/part-00003-eba73229-678b-412b-9a56-93a05c6fa933.c000.snappy.parquet, range: 0-7770537, partition values: [15]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  Executor:54 - Finished task 0.0 in stage 3.0 (TID 35). 1332 bytes result sent to driver
2018-11-06 08:38:39 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 36
2018-11-06 08:38:39 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 37
2018-11-06 08:38:39 INFO  Executor:54 - Running task 0.0 in stage 4.0 (TID 36)
2018-11-06 08:38:39 INFO  Executor:54 - Running task 1.0 in stage 4.0 (TID 37)
2018-11-06 08:38:39 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 38
2018-11-06 08:38:39 INFO  CoarseGrainedExecutorBackend:54 - Got assigned task 39
2018-11-06 08:38:39 INFO  Executor:54 - Running task 2.0 in stage 4.0 (TID 38)
2018-11-06 08:38:39 INFO  TorrentBroadcast:54 - Started reading broadcast variable 8
2018-11-06 08:38:39 INFO  Executor:54 - Running task 3.0 in stage 4.0 (TID 39)
2018-11-06 08:38:39 INFO  MemoryStore:54 - Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.3 KB, free 15.8 GB)
2018-11-06 08:38:39 INFO  TorrentBroadcast:54 - Reading broadcast variable 8 took 7 ms
2018-11-06 08:38:39 INFO  MemoryStore:54 - Block broadcast_8 stored as values in memory (estimated size 35.9 KB, free 15.8 GB)
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00003-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6667934, partition values: [3]
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=15/part-00000-eba73229-678b-412b-9a56-93a05c6fa933.c000.snappy.parquet, range: 0-7768490, partition values: [15]
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00001-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6671878, partition values: [3]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=22/part-00002-bf35044a-fffd-40a5-a43e-9c52af3b1eb4.c000.snappy.parquet, range: 0-5312487, partition values: [22]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=15/part-00002-eba73229-678b-412b-9a56-93a05c6fa933.c000.snappy.parquet, range: 0-7768268, partition values: [15]
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=22/part-00001-bf35044a-fffd-40a5-a43e-9c52af3b1eb4.c000.snappy.parquet, range: 0-5312357, partition values: [22]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=22/part-00000-bf35044a-fffd-40a5-a43e-9c52af3b1eb4.c000.snappy.parquet, range: 0-5300468, partition values: [22]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00000-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6672915, partition values: [3]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  Executor:54 - Finished task 3.0 in stage 4.0 (TID 39). 1332 bytes result sent to driver
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=6/part-00001-157efac3-2e9c-44f0-b573-223e59567c0e.c000.snappy.parquet, range: 0-5667602, partition values: [6]
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00004-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6669852, partition values: [3]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  Executor:54 - Finished task 0.0 in stage 4.0 (TID 36). 3861 bytes result sent to driver
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=6/part-00000-157efac3-2e9c-44f0-b573-223e59567c0e.c000.snappy.parquet, range: 0-5666691, partition values: [6]
2018-11-06 08:38:39 INFO  FileScanRDD:54 - Reading File path: hdfs://namenode:8020/data_hub/data/lake/client-pohjola/dept1/data_spec_rim/yields/converted/ds_calypso/io_yields_convert_corr_id=3/part-00002-0365610f-f4d4-423f-a0fc-81fd3aaa746d.c000.snappy.parquet, range: 0-6669303, partition values: [3]
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  FilterCompat:71 - Filtering using predicate: and(noteq(InterestRatePercent, null), noteq(MarginalRatePercent, null))
2018-11-06 08:38:39 INFO  Executor:54 - Finished task 2.0 in stage 4.0 (TID 38). 5315 bytes result sent to driver
2018-11-06 08:38:39 INFO  Executor:54 - Finished task 1.0 in stage 4.0 (TID 37). 7256 bytes result sent to driver